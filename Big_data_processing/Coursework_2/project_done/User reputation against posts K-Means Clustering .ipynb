{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "> This Notebook contains the implementation for data/feature extraction and our own K-Means cluster implementation for a two features. The code is divided among cells to help understand the code easily. Explainination of each cell is at the top of the cell.\n",
    "\n",
    "*NOTE: In order to run this code on a cluster do the following in the termianl first*\n",
    "\n",
    "**Environment Setup**\n",
    "\n",
    "Commands to be run in terminal:\n",
    "\n",
    "*Only need to run this command once*\n",
    "<font color=red>'pip install findspark --user'</font>\n",
    "\n",
    "*Set environment variable*\n",
    "<font color=red>'export SPARK_HOME=/usr/lib/spark'</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Import relevent libraries and initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "import xml.etree.ElementTree as ET\n",
    "SC = pyspark.SparkContext(appName=\"KMeans Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Setup input RDD's\n",
    "- We are using *posts* and *users* files from the stack overflow data.\n",
    "- Due to large amount of data and cluster taking alot of time to run the job, we took 0.5 of the *posts* data to process.\n",
    "- This code would work for larger dataset as well but due to load on cluster and time constraint to optimize the code even further, we have used a fraction of the data set for now. \n",
    "- The commented out lines of code in the following cell would be used to get complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Full Data Set\n",
    "#posts = SC.textFile(\"/data/stackoverflow/Posts\")\n",
    "users = SC.textFile(\"/data/stackoverflow/Users\")\n",
    "\n",
    "###Fraction of data set with '0.5' meaning 50% \n",
    "posts = SC.textFile(\"/data/stackoverflow/Posts\").sample(False,0.5,12345)\n",
    "#users = SC.textFile(\"/data/stackoverflow/Users\").sample(False,0.7,12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Data/Feature Extraction\n",
    "- Following cells contain the functions used to convert the raw data that is in XML format into a format understood by our algorithm. \n",
    "- The first cell contains the function used for feature extraction.\n",
    "- The second cell contains the transformations (maps and filters) used to get relevent RDD's for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOwnerId(input):\n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'OwnerUserId' in tree.attrib:\n",
    "            a = int(tree.attrib['OwnerUserId'])\n",
    "            return a\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def getUserId(input):\n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'Id' in tree.attrib:\n",
    "            a = int(tree.attrib['Id'])\n",
    "            return a\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def getUserReputation(input):\n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'Reputation' in tree.attrib:\n",
    "            a = int(tree.attrib['Reputation'])\n",
    "            return a\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Input RDD for 2D Clustering of user reputation score against number of posts they have ####################\n",
    "##Get number of posts for each users\n",
    "noOfPostsAgainstUser = posts.map(lambda post: (getOwnerId(post), 1))\\\n",
    ".map(lambda x: (x[0],x[1]))\\\n",
    ".filter(lambda x: x[0] is not None)\\\n",
    ".reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "##Get reputatition score for each user\n",
    "repAgainstUser = users.map(lambda post: (getUserId(post), getUserReputation(post)))\\\n",
    ".map(lambda x: (x[0],x[1]))\\\n",
    ".filter(lambda x: x[0] is not None and x[1] is not None)\n",
    "\n",
    "##Join posts count with their score for each user\n",
    "repAgainstPost = repAgainstUser.join(noOfPostsAgainstUser).map(lambda x: x[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Algorithm implementation:\n",
    "> We have a class defined as a KMeansModel2D which has the relevent functions to train itself on the data provided to it and other functions e.g a function that can be used to assign cluster to a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansModel2D:\n",
    "    ##Initialize the model with some cluster centers. \n",
    "    def __init__(self, centers):\n",
    "        self.centers = centers\n",
    "    \n",
    "    ##Function used to determine assigned cluster for a data point\n",
    "    def assignCluster(self,p):\n",
    "        import math\n",
    "        bestIndex = -1\n",
    "        closest = 100000\n",
    "        for i in range(len(self.centers)):\n",
    "            tempDist = math.sqrt((self.centers[i][0] - p[0])**2 + (self.centers[i][1] - p[1])**2 )\n",
    "            if tempDist < closest:\n",
    "                closest = tempDist\n",
    "                bestIndex = i\n",
    "                \n",
    "        return bestIndex\n",
    "    ##Method to calculate minimum distance of a point to the closest cluster\n",
    "    def getMinDistance(self, p):\n",
    "        import math\n",
    "        closest = 100000\n",
    "        \n",
    "        for i in range(len(self.centers)):\n",
    "            tempDist = math.sqrt( (self.centers[i][0] - p[0])**2 + (self.centers[i][1] - p[1])**2 )\n",
    "            if tempDist < closest:\n",
    "                closest = tempDist\n",
    "            \n",
    "        return tempDist\n",
    "    \n",
    "    ##Method to train the model with given data\n",
    "    def TrainModel(self,data):\n",
    "        ##Print the initially assigned clusters which should be random\n",
    "        print(\"Initial centers: \" + str(self.centers))\n",
    "        ##Run the algorithm until cluster movement(summed distance of updated centers and previous ones) in each\n",
    "        ##iteration is less then our threshold value (convergeDist) \n",
    "        convergeDist = float(20)\n",
    "        tempDist = float(100)\n",
    "        \n",
    "        while tempDist > convergeDist:\n",
    "            closests = data.map(lambda p: (self.assignCluster(list(p)), (p, 1)))\n",
    "            pointStats = closests.reduceByKey(\n",
    "            lambda p1_c1, p2_c2: ((p1_c1[0][0] + p2_c2[0][0],p1_c1[0][1] + p2_c2[0][1]), p1_c1[1] + p2_c2[1]))\n",
    "\n",
    "            newPoints = pointStats.mapValues(\n",
    "            lambda st: (st[0][0]/st[1], st[0][1]/st[1])).collect()\n",
    "            \n",
    "            sumDist = 0\n",
    "            for (iK, p) in newPoints:\n",
    "                import math\n",
    "                sumDist = sumDist + math.sqrt((self.centers[iK][0] - p[0])**2 + (self.centers[iK][1] - p[1])**2 )\n",
    "                self.centers[iK] = p\n",
    "            \n",
    "            tempDist = sumDist\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Instentiate KMeans class and train the model.\n",
    "- Create KMeans Class with three random data points.(We are clustering it into three clusters)\n",
    "- Train the model\n",
    "- Print the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeansModel2D(repAgainstPost.takeSample(False, 3,1))\n",
    "model.TrainModel(repAgainstPost)\n",
    "print(\"Final centers: \" + str(model.centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: Using our trained model, get the clustered data points and save it in output folder on our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = repAgainstPost.map(lambda p: (model.assignCluster(p), p))\n",
    "datapoints.saveAsTextFile('output/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6: Copy the output to local file system\n",
    "\n",
    "*NOTE: Run this command in terminal to save the output as text file*\n",
    "\n",
    "<font color=red>'hadoop fs -getmerge output/data UserRepAgainstPosts.txt'</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Step: Stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SC.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
