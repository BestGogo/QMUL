{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "> This Notebook contains the implementation for data/feature extraction and our own K-Means cluster implementation for a single feature. The code is divided among cells to help understand the code easily. Explainination of each cell is at the top of the cell.\n",
    "\n",
    "*NOTE: In order to run this code on a cluster do the following in the termianl first*\n",
    "\n",
    "**Environment Setup**\n",
    "\n",
    "Commands to be run in terminal:\n",
    "\n",
    "*Only need to run this command once*\n",
    "\n",
    "<font color=red>'pip install findspark --user'</font>\n",
    "\n",
    "*Set environment variable*\n",
    "\n",
    "<font color=red>'export SPARK_HOME=/usr/lib/spark'</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Import relevent libraries and initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "import xml.etree.ElementTree as ET\n",
    "SC = pyspark.SparkContext(appName=\"KMeans Implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Setup input RDD's\n",
    "- We are using *posts* data from the stack overflow.\n",
    "- Due to large amount of data and cluster taking alot of time to run the job, we took 0.5 of the *posts* data to process.\n",
    "- This code would work for larger dataset as well but due to load on cluster and time constraint to optimize the code even further, we have used a fraction of the data set for now. \n",
    "- The commented out lines of code in the following cell would be used to get complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Full Data Set\n",
    "#posts = SC.textFile(\"/data/stackoverflow/Posts\")\n",
    "\n",
    "###Fraction of data set with '0.5' meaning 50% \n",
    "posts = SC.textFile(\"/data/stackoverflow/Posts\").sample(False,0.005,12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Data/Feature Extraction\n",
    "- Following cells contain the functions used to convert the raw data that is in XML format into a format understood by our algorithm. \n",
    "- The first cell contains the function used for feature extraction.\n",
    "- The second and third cells contain the transformations (maps and filters) used to get relevent RDD's for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAcceptedAnswerIds(input):    \n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'AcceptedAnswerId' in tree.attrib:\n",
    "            return int(tree.attrib['AcceptedAnswerId'])\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "def getPostId(input):\n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'Id' in tree.attrib: \n",
    "            return int(tree.attrib['Id'])\n",
    "        else: \n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "import re\n",
    "def getBodyLength(input):\n",
    "    try:\n",
    "        tree = ET.fromstring(input)\n",
    "        if 'Body' in tree.attrib:\n",
    "            a = re.sub(r'\\<[^>]*\\>', '', tree.attrib['Body'])\n",
    "            return len(a)\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################Input RDD for 1D Clustering of length of accepted ansewers ###################################\n",
    "##Get accepted answer postID's \n",
    "answerIds = posts.map(getAcceptedAnswerIds)\\\n",
    ".filter(lambda f : f is not None)\n",
    "\n",
    "##Send the list to each node\n",
    "IdsToRetrieve = SC.broadcast(answerIds.collect())\n",
    "\n",
    "##Filter out the posts that are accepted answers\n",
    "acceptedAnswerPosts = posts.filter(lambda f : getPostId(f) in IdsToRetrieve.value)\n",
    "\n",
    "##Get the character length of the accepted answer posts\n",
    "clusterInput = acceptedAnswerPosts.map(getBodyLength)\\\n",
    ".filter(lambda f : f is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Algorithm implementation:\n",
    "> We have a class defined as a KMeansModel which has the relevent functions to train itself on the data provided to it and other functions e.g a function that can be used to assign cluster to a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansModel:\n",
    "    ##Initialize the model with some cluster centers.\n",
    "    def __init__(self, centers):\n",
    "        self.centers = centers\n",
    "    \n",
    "    ##Function used to determine assigned cluster for a data point\n",
    "    def assignCluster(self,p):\n",
    "        bestIndex = 0\n",
    "        closest = 100000\n",
    "        for i in range(len(self.centers)):\n",
    "            tempDist = (p - self.centers[i]) ** 2\n",
    "            if tempDist < closest:\n",
    "                closest = tempDist\n",
    "                bestIndex = i\n",
    "        return bestIndex\n",
    "    \n",
    "    ##Method to calculate minimum distance of a point to the closest cluster\n",
    "    def getMinDistance(self, p):\n",
    "        closest = 100000\n",
    "        for i in range(len(self.centers)):\n",
    "            tempDist = (p - self.centers[i]) ** 2\n",
    "            if tempDist < closest:\n",
    "                closest = tempDist\n",
    "            \n",
    "        return tempDist\n",
    "    \n",
    "    ##Method to train the model with given data\n",
    "    def TrainModel(self,data):\n",
    "        \n",
    "        ##Print the initially assigned clusters which should be random\n",
    "        print(\"Initial centers: \" + str(self.centers))\n",
    "        \n",
    "        ##Run the algorithm until cluster movement(summed distance of updated centers and previous ones) in each\n",
    "        ##iteration is less then our threshold value (convergeDist) \n",
    "        convergeDist = float(10)\n",
    "        tempDist = float(100)\n",
    "        \n",
    "        while tempDist > convergeDist:\n",
    "            assignedPoints = data.map(lambda p: (self.assignCluster(p), (p, 1)))\n",
    "\n",
    "            pointStats = assignedPoints.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\n",
    "\n",
    "            newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "            \n",
    "            sumDist = 0\n",
    "            for (iK, p) in newCenters:\n",
    "                sumDist = sumDist + ((self.centers[iK] - p) ** 2)\n",
    "                self.centers[iK] = p\n",
    "            \n",
    "            tempDist = sumDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Instentiate KMeans class and train the model.\n",
    "- Create KMeans Class with three random data points.(We are clustering it into three clusters)\n",
    "- Train the model\n",
    "- Print the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial centers: [175, 2558, 1131]\n"
     ]
    }
   ],
   "source": [
    "model = KMeansModel(clusterInput.takeSample(False, 3,1))\n",
    "model.TrainModel(clusterInput)\n",
    "print(\"Final centers: \" + str(model.centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5: Using our trained model, get the clustered data points and save it in output folder on our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = clusterInput.map(lambda p: (model.assignCluster(p), p))\n",
    "datapoints.saveAsTextFile('output/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6: Copy the output to local file system\n",
    "\n",
    "*NOTE: Run this command in terminal to save the output as text file*\n",
    "\n",
    "<font color=red>'hadoop fs -getmerge output/data PostLengthData.txt'</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Step: Stop spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SC.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
