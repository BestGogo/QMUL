{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodecsv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import csv                               # csv reader\n",
    "#nltk.download()\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/Shared/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label))\n",
    "        del preprocessedData[0]\n",
    "        del rawData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    rid = reviewLine[0]\n",
    "    rtext = reviewLine[8]\n",
    "    rlabel = reviewLine[1]\n",
    "    return (rid, rtext, rlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Text processing with Scikit-Learn, basics\n",
    "# Creating a vectorizer that can be used to extract a bag of words\n",
    "# representation from documents\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "# Input: a string of one review\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = word_tokenize(text)\n",
    "    b = []\n",
    "    for word in text:\n",
    "        if word.isalpha(): # removing punctuation\n",
    "            if word not in stop_words: # removing stopwords or \"too common\" words\n",
    "                word = word.lower() # converting all letters to lower case\n",
    "                word = wordnet_lemmatizer.lemmatize(word)\n",
    "                word = stemmer.stem(word) # Using standart stemmer from the nltk\n",
    "                b.append(word)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "# Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVector = {}\n",
    "    for token in tokens:\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1.0\n",
    "        else:\n",
    "            featureVector[token] = float(featureVector[token] + 1)\n",
    "            \n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1.0\n",
    "        else:\n",
    "            featureDict[token] = float(featureDict[token] + 1)\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(Pipeline(memory=None,\n",
       "     steps=[('svc', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(trainData)\n",
    "trainClassifier(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    for i in range(0, len(dataset), foldSize):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"fold start %d foldSize %d\" % (i, foldSize))\n",
    "        myTestData = dataset[i:i+foldSize]\n",
    "        myTrainData = dataset[:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(myTrainData)\n",
    "        y_true = list(map(lambda x: x[1], myTestData))\n",
    "        y_pred = classifier.classify_many(map(lambda x: x[0], myTestData))\n",
    "        results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold start 0 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 1680 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 3360 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 5040 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 6720 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 8400 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 10080 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 11760 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 13440 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 15120 foldSize 1680\n",
      "Training Classifier...\n",
      "[(0.6042075023272598, 0.6035714285714285, 0.6033960950316696, None), (0.5933755386880387, 0.593452380952381, 0.5934120693591681, None), (0.5988973858274369, 0.5988095238095238, 0.598513578363463, None), (0.6184027413834805, 0.6178571428571429, 0.6177433937430394, None), (0.6092167173347504, 0.6089285714285714, 0.6084652684419097, None), (0.6215110516368766, 0.6214285714285714, 0.621117016090404, None), (0.6363330897038586, 0.6351190476190476, 0.6349937620994995, None), (0.5977839763732843, 0.5946428571428571, 0.5929949215731698, None), (0.6234030032467532, 0.6214285714285714, 0.621334676378435, None), (0.6152600833691747, 0.6154761904761905, 0.6149827986373154, None)]\n"
     ]
    }
   ],
   "source": [
    "folds = 10\n",
    "shuffle(trainData)\n",
    "results = []\n",
    "foldSize = int(len(trainData)/folds)\n",
    "for i in range(0, len(trainData), foldSize):\n",
    "    # insert code here that trains and tests on the 10 folds of data in the trainData\n",
    "    print(\"fold start %d foldSize %d\" % (i, foldSize))\n",
    "    myTestData = trainData[i:i+foldSize]\n",
    "    myTrainData = trainData[:i] + trainData[i+foldSize:]\n",
    "    classifier = trainClassifier(myTrainData)\n",
    "    y_true = list(map(lambda x: x[1], myTestData))\n",
    "    y_pred = classifier.classify_many(map(lambda x: x[0], myTestData))\n",
    "    results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Make use of the given functions trainClassifier and predictLabels \n",
    "to do the cross-validation. Make sure that your program stores the (average) precision, \n",
    "recall, f1 score, and accuracy of your classifier in a variable cv_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples:  16800 Features:  20924\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)))\n",
    "print(\"Preparing the dataset...\")\n",
    "\n",
    "loadData(reviewPath)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)))\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Preparing training and test data...\")\n",
    "\n",
    "splitData(0.8)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)))\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Training Samples: \", len(trainData), \"Features: \", len(featureDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'even': 1.0, 'american': 1.0, 'may': 1.0, 'would': 1.0, 'back': 1.0, 'orient': 1.0, 'medium': 2.0, 'compani': 1.0, 'differ': 1.0, 'should': 1.0, 'though': 1.0, 'send': 1.0, 'fit': 1.0, 'larg': 1.0, 'bodi': 1.0, 'type': 1.0, 'come': 1.0, 'standard': 1.0, 'small': 2.0, 'ethnic': 1.0, 'realiz': 1.0, 'need': 1.0, 'order': 1.0, 'look': 1.0, 'i': 2.0, 'hen': 1.0}, '__label2__')]\n"
     ]
    }
   ],
   "source": [
    "print(trainData[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold start 0 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 1680 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 3360 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 5040 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 6720 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 8400 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 10080 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 11760 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 13440 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 15120 foldSize 1680\n",
      "Training Classifier...\n",
      "[(0.6170947648264996, 0.6172619047619048, 0.6170993542151644, None), (0.6207733449158352, 0.6208333333333333, 0.6207062890573062, None), (0.5973646411685349, 0.5964285714285714, 0.5963244683942054, None), (0.6020615408785907, 0.6017857142857143, 0.6016622326119451, None), (0.6226769803785436, 0.6196428571428572, 0.6191689327665767, None), (0.6058702416279488, 0.6059523809523809, 0.605841730465693, None), (0.6016017407253402, 0.6, 0.5995689166193987, None), (0.6034181373990798, 0.6029761904761904, 0.6028719415119285, None), (0.6134073565323565, 0.6136904761904762, 0.6132822348437273, None), (0.6042558764577259, 0.6041666666666666, 0.6037375695263779, None)]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(trainData, 10)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n"
     ]
    }
   ],
   "source": [
    "classifier = trainClassifier(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_results = np.asarray(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6170947648264996 0.6172619047619048 0.6170993542151644 None]\n",
      " [0.6207733449158352 0.6208333333333333 0.6207062890573062 None]\n",
      " [0.5973646411685349 0.5964285714285714 0.5963244683942054 None]\n",
      " [0.6020615408785907 0.6017857142857143 0.6016622326119451 None]\n",
      " [0.6226769803785436 0.6196428571428572 0.6191689327665767 None]\n",
      " [0.6058702416279488 0.6059523809523809 0.605841730465693 None]\n",
      " [0.6016017407253402 0.6 0.5995689166193987 None]\n",
      " [0.6034181373990798 0.6029761904761904 0.6028719415119285 None]\n",
      " [0.6134073565323565 0.6136904761904762 0.6132822348437273 None]\n",
      " [0.6042558764577259 0.6041666666666666 0.6037375695263779 None]]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average precision is 0.6088524624910455\n",
      "Current average recall is 0.6082738095238096\n",
      "Current average fscore is 0.6080263670012324\n",
      "Current average fscore is None\n"
     ]
    }
   ],
   "source": [
    "print(\"Current average precision is \" + str(np.mean(cv_results[:,0], axis=0)))\n",
    "print(\"Current average recall is \" + str(np.mean(cv_results[:,1], axis=0)))\n",
    "print(\"Current average fscore is \" + str(np.mean(cv_results[:,2], axis=0)))\n",
    "print(\"Current average fscore is None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
